{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# \uc5f0\uad6c\uc790\uc5d0\uac8c \ub9de\ub294 \ucf00\ub77c\uc2a4 \uc18c\uac1c\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/04/01<br>\n",
    "**Last modified:** 2020/04/28<br>\n",
    "**Description:** \ucf00\ub77c\uc2a4\uc640 TF 2.0\uc73c\ub85c \ub525\ub7ec\ub2dd \uc5f0\uad6c\ub97c \ud558\uae30 \uc704\ud574 \uc54c\uc544\uc57c \ud560 \ubaa8\ub4e0 \uac83."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## \uc124\uc815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## \uc18c\uac1c\n",
    "\n",
    "\uba38\uc2e0\ub7ec\ub2dd \uc5f0\uad6c\uc790\uc778\uac00\uc694?\n",
    "NeurIPS\uc5d0 \ub17c\ubb38\uc744 \uc81c\ucd9c\ud558\uace0 \ucef4\ud4e8\ud130 \ube44\uc804\uc774\ub098 \uc790\uc5f0\uc5b4 \ucc98\ub9ac \ubd84\uc57c\uc5d0\uc11c \ucd5c\uace0\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\ub824\uace0 \ud558\ub098\uc694?\n",
    "\uc774 \uac00\uc774\ub4dc\uc5d0\uc11c \ucf00\ub77c\uc2a4 API\uc758 \ud575\uc2ec \uac1c\ub150\uc744 \uc18c\uac1c\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n",
    "\n",
    "\uc774 \uac00\uc774\ub4dc\uc5d0\uc11c \ub2e4\uc74c \ub0b4\uc6a9\uc744 \ubc30\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n",
    "\n",
    "- `Layer` \ud074\ub798\uc2a4\ub97c \uc0c1\uc18d\ud558\uc5ec \uce35\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "- `GradientTape`\uc73c\ub85c \uadf8\ub808\uc774\ub514\uc5b8\ud2b8(gradient)\ub97c \uacc4\uc0b0\ud558\uace0 \uc800\uc218\uc900 \ud6c8\ub828 \ubc18\ubcf5\ubb38\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "- `add_loss()` \uba54\uc11c\ub4dc\ub85c \uce35\uc5d0\uc11c \ub9cc\ub4e0 \uc190\uc2e4\uc744 \uae30\ub85d\ud569\ub2c8\ub2e4.\n",
    "- \uc800\uc218\uc900 \ud6c8\ub828 \ubc18\ubcf5\ubb38\uc5d0\uc11c \uce21\uc815 \uc9c0\ud45c\ub97c \uae30\ub85d\ud569\ub2c8\ub2e4.\n",
    "- `tf.function`\uc73c\ub85c \ucef4\ud30c\uc77c\ud558\uc5ec \uc2e4\ud589 \uc18d\ub3c4\ub97c \ub192\uc785\ub2c8\ub2e4.\n",
    "- \ud6c8\ub828 \ubaa8\ub4dc\ub098 \ucd94\ub860 \ubaa8\ub4dc\ub85c \uce35\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4.\n",
    "- \ucf00\ub77c\uc2a4 \ud568\uc218\ud615 API\n",
    "\n",
    "\ubcc0\uc774\ud615 \uc624\ud1a0\uc778\ucf54\ub354(Variational Autoencoder)\uc640 \ud558\uc774\ud37c\ub124\ud2b8\uc6cc\ud06c(Hypernetwork)\n",
    "\ub450 \uac1c\uc758 \uc5d4\ub4dc-\ud22c-\uc5d4\ub4dc \uc5f0\uad6c \uc608\uc81c \ud1b5\ud574 \uc2e4\uc81c\ub85c \ucf00\ub77c\uc2a4 API\ub97c \uc0ac\uc6a9\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## `Layer` \ud074\ub798\uc2a4\n",
    "\n",
    "`Layer`\ub294 \ucf00\ub77c\uc2a4\uc758 \uae30\ucd08 \ucd94\uc0c1 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n",
    "`Layer`\ub294 \uc0c1\ud0dc(\uac00\uc911\uce58)\uc640 (`call` \uba54\uc11c\ub4dc\uc5d0\uc11c \uc815\uc758\ud55c) \uc77c\ubd80 \uacc4\uc0b0\uc744 \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n",
    "\n",
    "\uac04\ub2e8\ud55c \uce35\uc758 \uc608\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`Layer` \ud074\ub798\uc2a4 \uc778\uc2a4\ud134\uc2a4\ub97c \ud30c\uc774\uc36c \ud568\uc218\ucc98\ub7fc \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# \uce35\uc758 \uac1d\uccb4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "# \ud568\uc218\ucc98\ub7fc \uc0ac\uc6a9\ud598 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n",
    "# `call` \uba54\uc11c\ub4dc\uc5d0 \ud544\uc694\ud55c \ub370\uc774\ud130\ub97c \uc804\ub2ec\ud558\uba74\uc11c \ud638\ucd9c\ud569\ub2c8\ub2e4.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "(`__init__` \uba54\uc11c\ub4dc\uc5d0\uc11c \uc0dd\uc131\ud55c) \uac00\uc911\uce58 \ubcc0\uc218\ub294 \uc790\ub3d9\uc73c\ub85c `weights` \uc18d\uc131\uc5d0 \uae30\ub85d\ub429\ub2c8\ub2e4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\uae30\ubcf8\uc73c\ub85c \ub0b4\uc7a5\ub41c \uce35\uc774 \ub9ce\uc2b5\ub2c8\ub2e4.\n",
    "`Dense` \uce35, `Conv2D` \uce35, `LSTM` \uce35\uc774 \uc788\uace0\n",
    "`Conv3DTranspose`\uc774\ub098 `ConvLSTM2D`\uc640 \uac19\uc740 \ud654\ub824\ud55c \uce35\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n",
    "\uac00\ub2a5\ud558\uba74 \ub0b4\uc7a5\ub41c \uae30\ub2a5\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## \uac00\uc911\uce58 \uc0dd\uc131\n",
    "\n",
    "`add_weight` \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uba74 \uc190\uc27d\uac8c \uac00\uc911\uce58\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# \uce35\uc758 \uac1d\uccb4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# `build(input_shape)`\uc744 \ud638\ucd9c\ud558\uace0 \uac00\uc911\uce58\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## \uadf8\ub808\uc774\ub514\uc5b8\ud2b8\n",
    "\n",
    "`GradientTape` \ucee8\ud0dd\uc2a4\ud2b8 \uc548\uc5d0\uc11c \uce35\uc744 \ud638\ucd9c\ud558\uba74 \uc790\ub3d9\uc73c\ub85c \uce35 \uac00\uc911\uce58\uc758 \uadf8\ub808\uc774\ub514\uc5b8\ud2b8\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n",
    "\uc774 \uadf8\ub808\uc774\ub514\uc5b8\ud2b8\ub97c \uc0ac\uc6a9\ud574 \uc635\ud2f0\ub9c8\uc774\uc800 \uac1d\uccb4\ub97c \uc0ac\uc6a9\ud558\uac70\ub098 \uc218\ub3d9\uc73c\ub85c \uce35\uc758 \uac00\uc911\uce58\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n",
    "\ubb3c\ub860 \ud544\uc694\ud558\uba74 \uc5c5\ub370\uc774\ud2b8\ud558\uae30 \uc804\uc5d0 \uadf8\ub808\uc774\ub514\uc5b8\ud2b8\ub97c \uc218\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# \ub370\uc774\ud130\uc14b\uc744 \uc900\ube44\ud569\ub2c8\ub2e4.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# 10\uac1c\uc758 \uc720\ub2db(unit)\uc744 \uac00\uc9c4 (\uc704\uc5d0\uc11c \uc815\uc758\ud55c) \uc120\ud615 \uce35\uc758 \uac1d\uccb4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# \uc815\uc218 \ud0c0\uae43\uc744 \uae30\ub300\ud558\ub294 \ub85c\uc9c0\uc2a4\ud2f1 \uc190\uc2e4 \ud568\uc218 \uac1d\uccb4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# \uc635\ud2f0\ub9c8\uc774\uc800 \uac1d\uccb4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# \ub370\uc774\ud130\uc14b\uc758 \ubc30\uce58\ub97c \uc21c\ud68c\ud569\ub2c8\ub2e4.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "\n",
    "    # GradientTape\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # \uc815\ubc29\ud5a5 \uacc4\uc0b0\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # \ubc30\uce58\uc758 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # \uc190\uc2e4\uc5d0 \ub300\ud55c \uac00\uc911\uce58\uc758 \uadf8\ub808\uc774\ub514\uc5b8\ud2b8\ub97c \uc5bb\uc2b5\ub2c8\ub2e4.\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # \uc120\ud615 \uce35\uc758 \uac00\uc911\uce58\ub97c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # \ub85c\uae45\n",
    "    if step % 100 == 0:\n",
    "        print(\"\uc2a4\ud15d:\", step, \"\uc190\uc2e4:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Trainable and non-trainable weights\n",
    "\n",
    "Weights created by layers can be either trainable or non-trainable. They're\n",
    "exposed in `trainable_weights` and `non_trainable_weights` respectively.\n",
    "Here's a layer with a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # Create a non-trainable weight.\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Layers that own layers\n",
    "\n",
    "Layers can be recursively nested to create bigger computation blocks.\n",
    "Each layer will track the weights of its sublayers\n",
    "(both trainable and non-trainable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Note that our manually-created MLP above is equivalent to the following\n",
    "built-in option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Tracking losses created by layers\n",
    "\n",
    "Layers can create losses during the forward pass via the `add_loss()` method.\n",
    "This is especially useful for regularization losses.\n",
    "The losses created by sublayers are recursively tracked by the parent layers.\n",
    "\n",
    "Here's a layer that creates an activity regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Any model incorporating this layer will track this regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's use the loss layer in a MLP block.\n",
    "\n",
    "\n",
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "These losses are cleared by the top-level layer at the start of each forward\n",
    "pass -- they don't accumulate. `layer.losses` always contains only the losses\n",
    "created during the last forward pass. You would typically use these losses by\n",
    "summing them before computing your gradients when writing a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Losses correspond to the *last* forward pass.\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1  # No accumulation.\n",
    "\n",
    "# Let's demonstrate how to use these losses in a training loop.\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = mlp(x)\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        # Get gradients of weights wrt the loss.\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Keeping track of training metrics\n",
    "\n",
    "Keras offers a broad range of built-in metrics, like `tf.keras.metrics.AUC`\n",
    "or `tf.keras.metrics.PrecisionAtRecall`. It's also easy to create your\n",
    "own metrics in a few lines of code.\n",
    "\n",
    "To use a metric in a custom training loop, you would:\n",
    "\n",
    "- Instantiate the metric object, e.g. `metric = tf.keras.metrics.AUC()`\n",
    "- Call its `metric.udpate_state(targets, predictions)` method for each batch of data\n",
    "- Query its result via `metric.result()`\n",
    "- Reset the metric's state at the end of an epoch or at the start of an evaluation via\n",
    "`metric.reset_states()`\n",
    "\n",
    "Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Instantiate a metric object\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        # Update the state of the `accuracy` metric.\n",
    "        accuracy.update_state(y, logits)\n",
    "\n",
    "        # Update the weights of the model to minimize the loss value.\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # Logging the current accuracy value so far.\n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
    "\n",
    "    # Result the metric's state at the end of an epoch\n",
    "    accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In addition to this, similarly to the `self.add_loss()` method, you have access\n",
    "to an `self.add_metric()` method on layers. It tracks the average of\n",
    "whatever quantity you pass to it. You can reset the value of these metrics\n",
    "by calling `layer.reset_metrics()` on any layer or model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Compiled functions\n",
    "\n",
    "Running eagerly is great for debugging, but you will get better performance by\n",
    "compiling your computation into static graphs. Static graphs are a researcher's\n",
    "best friends. You can compile any function by wrapping it in a `tf.function`\n",
    "decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Create a training step function.\n",
    "\n",
    "\n",
    "@tf.function  # Make it fast.\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training mode & inference mode\n",
    "\n",
    "Some layers, in particular the `BatchNormalization` layer and the `Dropout`\n",
    "layer, have different behaviors during training and inference. For such layers,\n",
    "it is standard practice to expose a `training` (boolean) argument in the `call`\n",
    "method.\n",
    "\n",
    "By exposing this argument in `call`, you enable the built-in training and\n",
    "evaluation loops (e.g. fit) to correctly use the layer in training and\n",
    "inference modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class MLPWithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## The Functional API for model-building\n",
    "\n",
    "To build deep learning models, you don't have to use object-oriented programming all the\n",
    "time. All layers we've seen so far can also be composed functionally, like this (we call\n",
    "it the \"Functional API\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
    "# This is the deep learning equivalent of *declaring a type*.\n",
    "# The shape argument is per-sample; it does not include the batch size.\n",
    "# The functional API focused on defining per-sample transformations.\n",
    "# The model we create will automatically batch the per-sample transformations,\n",
    "# so that it can be called on batches of data.\n",
    "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")\n",
    "\n",
    "# We call layers on these \"type\" objects\n",
    "# and they return updated types (new shapes/dtypes).\n",
    "x = Linear(32)(inputs)  # We are reusing the Linear layer we defined earlier.\n",
    "x = Dropout(0.5)(x)  # We are reusing the Dropout layer we defined earlier.\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# A functional `Model` can be defined by specifying inputs and outputs.\n",
    "# A model is itself a layer like any other.\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# A functional model already has weights, before being called on any data.\n",
    "# That's because we defined its input shape in advance (in `Input`).\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# Let's call our model on some data, for fun.\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)\n",
    "\n",
    "# You can pass a `training` argument in `__call__`\n",
    "# (it will get passed down to the Dropout layer).\n",
    "y = model(tf.ones((2, 16)), training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The Functional API tends to be more concise than subclassing, and provides a few other\n",
    "advantages (generally the same advantages that functional, typed languages provide over\n",
    "untyped OO development). However, it can only be used to define DAGs of layers --\n",
    "recursive networks should be defined as Layer subclasses instead.\n",
    "\n",
    "Learn more about the Functional API [here](/guides/functional_api/).\n",
    "\n",
    "In your research workflows, you may often find yourself mix-and-matching OO models and\n",
    "Functional models.\n",
    "\n",
    "Note that the `Model` class also features built-in training & evaluation loops\n",
    "(`fit()` and `evaluate()`). You can always subclass the `Model` class\n",
    "(it works exactly like subclassing `Layer`) if you want to leverage these loops\n",
    "for your OO models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## End-to-end experiment example 1: variational autoencoders.\n",
    "\n",
    "Here are some of things you've learned so far:\n",
    "\n",
    "- A `Layer` encapsulate a state (created in `__init__` or `build`) and some computation\n",
    "(defined in `call`).\n",
    "- Layers can be recursively nested to create new, bigger computation blocks.\n",
    "- You can easily write highly hackable training loops by opening a\n",
    "`GradientTape`, calling your model inside the tape's scope, then retrieving\n",
    "gradients and applying them via an optimizer.\n",
    "- You can speed up your training loops using the `@tf.function` decorator.\n",
    "- Layers can create and track losses (typically regularization losses) via\n",
    "`self.add_loss()`.\n",
    "\n",
    "Let's put all of these things together into an end-to-end example: we're going to\n",
    "implement a Variational AutoEncoder (VAE). We'll train it on MNIST digits.\n",
    "\n",
    "Our VAE will be a subclass of `Layer`, built as a nested composition of layers that\n",
    "subclass `Layer`. It will feature a regularization loss (KL divergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Below is our model definition.\n",
    "\n",
    "First, we have an `Encoder` class, which uses a `Sampling` layer to map a MNIST digit to\n",
    "a latent-space triplet `(z_mean, z_log_var, z)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, we have a `Decoder` class, which maps the probabilistic latent space coordinates\n",
    "back to a MNIST digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, our `VariationalAutoEncoder` composes together an encoder and a decoder, and\n",
    "creates a KL divergence regularization loss via `add_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class VariationalAutoEncoder(layers.Layer):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, let's write a training loop. Our training step is decorated with a `@tf.function` to\n",
    "compile into a super fast graph function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Our model.\n",
    "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed = vae(x)  # Compute input reconstruction.\n",
    "        # Compute loss.\n",
    "        loss = loss_fn(x, reconstructed)\n",
    "        loss += sum(vae.losses)  # Add KLD term.\n",
    "    # Update the weights of the VAE.\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, x in enumerate(dataset):\n",
    "    loss = training_step(x)\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As you can see, building and training this type of model in Keras\n",
    "is quick and painless.\n",
    "\n",
    "Now, you may find that the code above is somewhat verbose: we handle every little detail\n",
    "on our own, by hand. This gives the most flexibility, but it's also a bit of work.\n",
    "\n",
    "Let's take a look at what the Functional API version of\n",
    "our VAE looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Much more concise, right?\n",
    "\n",
    "By the way, Keras also features built-in training & evaluation loops on its `Model` class\n",
    "(`fit()` and `evaluate()`). Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.map(lambda x: (x, x))  # Use x_train as both inputs & targets\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "# Configure the model for training.\n",
    "vae.compile(optimizer, loss=loss_fn)\n",
    "\n",
    "# Actually training the model.\n",
    "vae.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The use of the Functional API and `fit` reduces our example from 65 lines to 25 lines\n",
    "(including model definition & training). The Keras philosophy is to offer you\n",
    "productivity-boosting features like\n",
    "these, while simultaneously empowering you to write everything yourself to gain absolute\n",
    "control over every little detail. Like we did in the low-level training loop two\n",
    "paragraphs earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## End-to-end experiment example 2: hypernetworks.\n",
    "\n",
    "Let's take a look at another kind of research experiment: hypernetworks.\n",
    "\n",
    "A hypernetwork is a deep neural network whose weights are generated by another network\n",
    "(usually smaller).\n",
    "\n",
    "Let's implement a really trivial hypernetwork: we'll use a small 2-layer network  to\n",
    "generate the weights of a larger 3-layer network.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dim = 784\n",
    "classes = 10\n",
    "\n",
    "# This is the model we'll actually use to predict labels (the hypernetwork).\n",
    "outer_model = keras.Sequential(\n",
    "    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes),]\n",
    ")\n",
    "\n",
    "# It doesn't need to create its own weights, so let's mark its layers\n",
    "# as already built. That way, calling `outer_model` won't create new variables.\n",
    "for layer in outer_model.layers:\n",
    "    layer.built = True\n",
    "\n",
    "# This is the number of weight coefficients to generate. Each layer in the\n",
    "# hypernetwork requires output_dim * input_dim + output_dim coefficients.\n",
    "num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)\n",
    "\n",
    "# This is the model that generates the weights of the `outer_model` above.\n",
    "inner_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is our training loop. For each batch of data:\n",
    "\n",
    "- We use `inner_model` to generate an array of weight coefficients, `weights_pred`\n",
    "- We reshape these coefficients into kernel & bias tensors for the `outer_model`\n",
    "- We run the forward pass of the `outer_model` to compute the actual MNIST predictions\n",
    "- We run backprop through the weights of the `inner_model` to minimize the\n",
    "final classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "\n",
    "# We'll use a batch size of 1 for this experiment.\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict weights for the outer model.\n",
    "        weights_pred = inner_model(x)\n",
    "\n",
    "        # Reshape them to the expected shapes for w and b for the outer model.\n",
    "        # Layer 0 kernel.\n",
    "        start_index = 0\n",
    "        w0_shape = (input_dim, 64)\n",
    "        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]\n",
    "        w0 = tf.reshape(w0_coeffs, w0_shape)\n",
    "        start_index += np.prod(w0_shape)\n",
    "        # Layer 0 bias.\n",
    "        b0_shape = (64,)\n",
    "        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]\n",
    "        b0 = tf.reshape(b0_coeffs, b0_shape)\n",
    "        start_index += np.prod(b0_shape)\n",
    "        # Layer 1 kernel.\n",
    "        w1_shape = (64, classes)\n",
    "        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]\n",
    "        w1 = tf.reshape(w1_coeffs, w1_shape)\n",
    "        start_index += np.prod(w1_shape)\n",
    "        # Layer 1 bias.\n",
    "        b1_shape = (classes,)\n",
    "        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]\n",
    "        b1 = tf.reshape(b1_coeffs, b1_shape)\n",
    "        start_index += np.prod(b1_shape)\n",
    "\n",
    "        # Set the weight predictions as the weight variables on the outer model.\n",
    "        outer_model.layers[0].kernel = w0\n",
    "        outer_model.layers[0].bias = b0\n",
    "        outer_model.layers[1].kernel = w1\n",
    "        outer_model.layers[1].bias = b1\n",
    "\n",
    "        # Inference on the outer model.\n",
    "        preds = outer_model(x)\n",
    "        loss = loss_fn(y, preds)\n",
    "\n",
    "    # Train only inner model.\n",
    "    grads = tape.gradient(loss, inner_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, inner_model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_step(x, y)\n",
    "\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Implementing arbitrary research ideas with Keras is straightforward and highly\n",
    "productive. Imagine trying out 25 ideas per day (20 minutes per experiment on average)!\n",
    "\n",
    "Keras has been designed to go from idea to results as fast as possible, because we\n",
    "believe this is\n",
    "the key to doing great research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We hope you enjoyed this quick introduction. Let us know what you build with Keras!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_to_keras_for_researchers",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}